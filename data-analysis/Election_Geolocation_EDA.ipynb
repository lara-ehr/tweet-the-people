{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Election Tweets: Geolocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand just how real the \"red state/blue state\" divide is on the tweet level, I need some geolocation data. Since only very few users (<2%) activate the geotagging feature on their user accounts, I'll need to get this information from other sources. I decided to extract this info from people's self-reported location (in their user profile), but now I want to see what the data looks like and what information I can extract to put towards modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('/Users/laraehrenhofer/Documents/Coding_Projects/git_repos/tweet-the-people-legacy/data/tweet_pg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding geolocation by numbers\n",
    "\n",
    "Some introductory questions:\n",
    "- How many tweets have geotagged location vs. location based on profile info?\n",
    "- Any differences by Republican/Democrat ticket?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geotagged vs. profile geolocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic cleanup is necessary as some of these locations were generated in development versions of the tweet streaming script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_no = pd.DataFrame(tweets.groupby(['loc_type', 'ticket']).count()['tweet_id']).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rows = ['bound_box_coords', 'no_loc', 'user_loc', 'geo_loc']\n",
    "drop_rows = [column for column in loc_no.index.values if column not in keep_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_no = loc_no.drop(drop_rows, axis=0)\n",
    "loc_no = pd.DataFrame(loc_no.to_records())\n",
    "loc_no.columns = ['loc_type', 'Democrat', 'Republican']\n",
    "loc_no['total'] = loc_no['Democrat'] + loc_no['Republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticket_percent(data, ticket):\n",
    "    colname_total = f'percent_of_total_{ticket}'\n",
    "    colname_ticket = f'percent_of_ticket_{ticket}'\n",
    "    data[colname_total] = data[ticket].apply(lambda x: round((x/sum(data['total']))*100, 2))\n",
    "    data[colname_ticket] = data[ticket].apply(lambda x: round((x/sum(data[ticket]))*100, 2))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = ['Democrat', 'Republican']\n",
    "\n",
    "for ticket in tickets:\n",
    "    loc_no = get_ticket_percent(loc_no, ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_no['total_percent'] = loc_no['total'].apply(lambda x: round((x/sum(loc_no['total']))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_type</th>\n",
       "      <th>Democrat</th>\n",
       "      <th>Republican</th>\n",
       "      <th>total</th>\n",
       "      <th>percent_of_total_Democrat</th>\n",
       "      <th>percent_of_ticket_Democrat</th>\n",
       "      <th>percent_of_total_Republican</th>\n",
       "      <th>percent_of_ticket_Republican</th>\n",
       "      <th>total_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bound_box_coords</td>\n",
       "      <td>851.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geo_loc</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_loc</td>\n",
       "      <td>78283.0</td>\n",
       "      <td>50747.0</td>\n",
       "      <td>129030.0</td>\n",
       "      <td>24.75</td>\n",
       "      <td>41.52</td>\n",
       "      <td>16.04</td>\n",
       "      <td>39.70</td>\n",
       "      <td>40.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_loc</td>\n",
       "      <td>109385.0</td>\n",
       "      <td>76665.0</td>\n",
       "      <td>186050.0</td>\n",
       "      <td>34.58</td>\n",
       "      <td>58.02</td>\n",
       "      <td>24.23</td>\n",
       "      <td>59.98</td>\n",
       "      <td>58.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loc_type  Democrat  Republican     total  \\\n",
       "0  bound_box_coords     851.0       401.0    1252.0   \n",
       "1           geo_loc      12.0         3.0      15.0   \n",
       "2            no_loc   78283.0     50747.0  129030.0   \n",
       "3          user_loc  109385.0     76665.0  186050.0   \n",
       "\n",
       "   percent_of_total_Democrat  percent_of_ticket_Democrat  \\\n",
       "0                       0.27                        0.45   \n",
       "1                       0.00                        0.01   \n",
       "2                      24.75                       41.52   \n",
       "3                      34.58                       58.02   \n",
       "\n",
       "   percent_of_total_Republican  percent_of_ticket_Republican  total_percent  \n",
       "0                         0.13                          0.31           0.40  \n",
       "1                         0.00                          0.00           0.00  \n",
       "2                        16.04                         39.70          40.79  \n",
       "3                        24.23                         59.98          58.81  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "1. Less than half a percent of this tweet database has 'ground-truth' geotagged location data.\n",
    "2. 40% has no location at all.\n",
    "3. But between geotagging and user profile location data, we still have close to 60% of the dataset available.\n",
    "4. Initially it looks like there's a partisan divide in terms of people's provision of geolocation data (looking at the `percent_of_total` columns). However, breaking it down into percent by ticket suggests that there isn't an enormous contrast between the two populations after all, this is just an artefact of there being slightly more Democrat than Republican tweets overall. (Could determine further statistical information concerning probability of having a location based on ticket using a logistic regression here but it doesn't seem interesting enough to warrant the extra attention.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quality check\n",
    "\n",
    "I used the `geocoder` package to get over 42k locations based on people's user profiles. While some folks' self-reported location is plausible (\"Milwaukee-Chicago\"), other locations are less straightforward to map onto real-world locations (\"Marvel Universe\", \"hell since 2016\", \"God's Country\", \"Always butter the Pan\"). What did `geocoder` make of these less plausible locations?\n",
    "\n",
    "How to check: Sample 1000 locations and manually check. (I want a GUI for this and am doing it in good old Excel.)\n",
    "- What proportion of these is a joke location? This will help us get a sense of the amount of noise in the data.\n",
    "- What does `geocoder` make of the joke locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45219"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_locs = list(tweets['location'].unique())\n",
    "len(list_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out just the tweets that have a profile-based location\n",
    "\n",
    "loc_tweets = tweets[tweets['loc_type'] == 'user_loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sample = loc_tweets.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sample = loc_sample.drop([column for column in loc_sample.columns if column not in ['location', 'loc_type', 'us_state', 'loc_lat', 'loc_lon']], axis=1)\n",
    "loc_sample.to_csv('./location_random_sample.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual data annotation of random sample:**\n",
    "\n",
    "1. Binary manual classification into feature `unclear_loc` (0, 1). Some examples of `unclear_loc == 1` in this particular sample:\n",
    "    - a `location: Right now? Arkansas`\n",
    "    - b `Earth`\n",
    "    - c `In Trump's Nightmares`\n",
    "    - d `COviNGTON va BAbY!!!!`\n",
    "    - e `Nicht Bielefeld`\n",
    "    - f `None of your business`\n",
    "    - g `NoVa`\n",
    "    - h `The Dirty South-GA`\n",
    "2. Binary manual classification into feature `implausible_assigned_loc` (0, 1). My highly subjective criteria were:\n",
    "    - `implausible_assigned_loc == 0`: if `geocoder` managed to get correct information out of an unclear location (e.g. assigning location `Arkansas` to example a, `Virginia` to example d), or classifies it as `other` (e.g. assigning `other` to example e).\n",
    "    - `implausible_assigned_loc == 1`: odder assignments get a rating of 1, e.g. assigning location b to Texas, c to Maryland, f to Washington, or g to Ohio (\"NoVa\" is short for Northern Virginia), or failing to get location information where technically present (e.g. assigning `other` to example h).\n",
    "    \n",
    "**Next up:** let's get a sense of the scale of the noise.\n",
    "1. What percentage of locations are unclear?\n",
    "2. Of these, what percentage are implausible and therefore likely erroneous?\n",
    "3. What's the likely percentage of erroneous assignments overall?\n",
    "\n",
    "**Update:** adding 2nd sample for higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sample_annotated1 = pd.read_csv('./location_random_sample_13012021.csv')\n",
    "loc_sample_annotated2 = pd.read_csv('./location_random_sample_withhashtags_14012021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sample_annotated1['hashtags'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sample_annotated = pd.concat([loc_sample_annotated1, loc_sample_annotated2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.6"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_unclear = (sum(loc_sample_annotated['unclear_loc'])/len(loc_sample_annotated))*100\n",
    "percent_unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.816326530612244"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_implausible = (sum(loc_sample_annotated['implausible_assigned_loc'])/len(loc_sample_annotated[loc_sample_annotated['unclear_loc'] == 1]))*100\n",
    "percent_implausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_erroneous_total = (sum(loc_sample_annotated['implausible_assigned_loc'])/len(loc_sample_annotated))*100\n",
    "percent_erroneous_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions from manual data quality check\n",
    "\n",
    "In this sample, nearly one-fifth of locations were unclear; 40% of those were not assigned a correct location by `geocoder`. Overall, this results in an 8% rate of poor location information.\n",
    "\n",
    "On the flip side, that's over 90% accuracy in `geocoder`'s ability to detect a user's location!\n",
    "\n",
    "**How can we use this to predict location for tweets that didn't include user location?**\n",
    "\n",
    "Options from various domains (which could also help impute missing location data for the users who provided none at all):\n",
    "\n",
    "1. **Hashtag tracking:** If the tweet contains a hashtag, in which state is this hashtag most popular?\n",
    "    - Given a particular hashtag, what's the likelihood of it being tweeted from a specific state? Or at a somewhat zoomed out level, from a red or blue state (rather than the exact state)?\n",
    "    - Predict the state from hashtags?\n",
    "        - This is somewhat risky, as the model will be learning from data whose labelling is only 92% accurate.\n",
    "        - To avoid the problems with poorly annotated data: Are there enough geotagged tweets containing hashtags in order to build a  model restricted to just this data subset, and then extrapolate from there to the wider dataset?\n",
    "            - Can we augment this data set through human-annotated reference samples?\n",
    "        - Otherwise: If we accept the 8% noise factor, can we build a more robust model using all of the location data we have (from either geotagging or user profile info) and use it to extrapolate to tweets that didn't have a user location?\n",
    "        - Note: Some subset of the data that contains no location information also does not contain hashtags. How much data will be left unlabelled through this approach?\n",
    "    \n",
    "2. **Social network clustering:** Can we interpolate a user's location from the people they follow and their locations?\n",
    "    - The dataset contains tweets from >220k unique users\n",
    "    - Steps would be:\n",
    "        - Grab user's followed accounts (Twitter API limits this to 5k followers at a time, this seems like more than enough)\n",
    "        - Get those users' locations where possible (this will have the same problem with noise in self-reported locations)\n",
    "        - Interpolation: copy location of the account that a user most frequently interacts with? Does Twitter provide this data in some summary form? If so, is the location of a high-interaction account actually a good proxy for a user's location? (Boils down to: do people interact more with people who live close to them? Probably some do and some don't.)\n",
    "        \n",
    "        \n",
    "Given the issues with Twitter API limits, I'm going to try hashtag tracking first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Location Interpolation Using Hashtags\n",
    "\n",
    "Logic/assumptions for this interpolation model:\n",
    "\n",
    "1. The use of a hashtag in a tweet predicts that tweet's partisan lean.\n",
    "    - E.g. #StopTheSteal is a pro-Republican hashtag, #Vote is a pro-Democrat hashtag.\n",
    "2. Partisan lean predicts geolocation.\n",
    "    - Pro-Trump hashtagged tweets are likely to come from pro-Trump states.\n",
    "3. Therefore, hashtag predicts location.\n",
    "\n",
    "\n",
    "Building the model:\n",
    "- Training and testing the model: I'll use the ground-truthed data that I have from the geotagged tweets. Since this is an extremely small data set, I'll augment with a fresh batch of human-annotated data.\n",
    "- Model type: LogReg classifier that predicts red state/blue state/lean red/lean blue based on hashtag feature\n",
    "- Geolocation based on user profile was over 90% accurate. If I can build a model that reaches 80% accuracy of predicting location from hashtags, I'll be happy.\n",
    "    - Lower threshold chosen because of the Poisson distribution in hashtag frequency, meaning that while some hashtags will have high degree of correlation with location (e.g. #StopTheSteal) less frequent ones will be noisier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking assumption 1: Do specific hashtags correlate with a tweet's partisan lean?\n",
    "\n",
    "How to check this:\n",
    "- Define \"partisan lean\"\n",
    "- Check correlations of the most frequent hashtags with degree of lean\n",
    "\n",
    "**Defining Partisan Lean**\n",
    "\n",
    "The basic idea:\n",
    "\n",
    "- If a tweet's sentiment is above the average daily score for tweets about a particular ticket, it is pro that ticket. If it's below the average, it's anti that ticket.\n",
    "    - Example: a tweet about Donald Trump that has a sentiment of 0.7 on a day when the average sentiment of Trump-related tweets is 0.2 has a partisan lean of 0.5 in Trump's favour.\n",
    "- This is essentially within-group normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking assumption 2: Does partisan lean correlate with red state/blue state location?\n",
    "\n",
    "Two things necessary here --\n",
    "\n",
    "- Define \"partisan lean\"\n",
    "- Define red & blue states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for location interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handles_hashtags(text):\n",
    "    '''\n",
    "    Returns separate lists of hashtags and user handles in the text\n",
    "    '''\n",
    "    handles = re.findall('\\B\\@\\w+', text)\n",
    "    hashtags = re.findall('\\B\\#\\w+', text)\n",
    "    return handles, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['handles', 'hashtags']] = tweets.apply(lambda row: pd.Series(get_handles_hashtags(row['text'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving for later\n",
    "\n",
    "tweets.to_csv('./tweets_loc_with_hashtags.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract geotagged tweets that also contain hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among the geotagged tweets, how many actually contain hashtags at all?\n",
    "\n",
    "geotag_tweets = tweets[tweets['loc_type'] == 'bound_box_coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotag_tweets['hashtags_present'] = [x if x != [] else np.nan for x in geotag_tweets['hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotag_hashtags = geotag_tweets.dropna()\n",
    "len(geotag_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new human-annotated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>us_state</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>other</td>\n",
       "      <td>[#JoeBiden, #1994crimebill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>[#Trump2020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>other</td>\n",
       "      <td>[#YouAndMeBoth, #BadAssWomen, #Vote, #VoteEarl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>Texas</td>\n",
       "      <td>[#Trump, #COVID, #GetWellSoon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>other</td>\n",
       "      <td>[#COVID19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313290</th>\n",
       "      <td>other</td>\n",
       "      <td>[#bullshitboy, #ScottyFromMarketing, #climatec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313759</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>[#GeorgeBush, #Constitution]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313866</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>[#Election2020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314238</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>[#BOSTON2024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315178</th>\n",
       "      <td>other</td>\n",
       "      <td>[#Japan, #JoeBiden, #US, #election]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              us_state                                           hashtags\n",
       "5257             other                        [#JoeBiden, #1994crimebill]\n",
       "5501          Virginia                                       [#Trump2020]\n",
       "6968             other  [#YouAndMeBoth, #BadAssWomen, #Vote, #VoteEarl...\n",
       "10115            Texas                     [#Trump, #COVID, #GetWellSoon]\n",
       "11300            other                                         [#COVID19]\n",
       "...                ...                                                ...\n",
       "313290           other  [#bullshitboy, #ScottyFromMarketing, #climatec...\n",
       "313759  South Carolina                       [#GeorgeBush, #Constitution]\n",
       "313866        Colorado                                    [#Election2020]\n",
       "314238   Massachusetts                                      [#BOSTON2024]\n",
       "315178           other                [#Japan, #JoeBiden, #US, #election]\n",
       "\n",
       "[374 rows x 2 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotag_hashtags = geotag_hashtags[['us_state', 'hashtags']]\n",
    "geotag_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_geotags = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in geotag_hashtags.iterrows():\n",
    "    states = pd.Series([row['us_state']]*len(row['hashtags']), name='state')\n",
    "    hashtags = pd.Series(row['hashtags'], name='hashtags')\n",
    "    new = pd.concat([states, hashtags], axis=1)\n",
    "    long_geotags = pd.concat([long_geotags, new])\n",
    "\n",
    "long_geotags = long_geotags.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6545"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_geotags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_geotags['hashtags'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_geotags['state'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>2335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                hashtags\n",
       "state                   \n",
       "Alabama               15\n",
       "Arizona              120\n",
       "Arkansas              50\n",
       "California           600\n",
       "Colorado              55\n",
       "Connecticut           50\n",
       "Delaware              20\n",
       "Florida              460\n",
       "Georgia              240\n",
       "Illinois              85\n",
       "Indiana               30\n",
       "Iowa                  60\n",
       "Kansas                90\n",
       "Kentucky              30\n",
       "Maryland             165\n",
       "Massachusetts         75\n",
       "Michigan              20\n",
       "Minnesota            185\n",
       "Mississippi            5\n",
       "Missouri              20\n",
       "Nevada               115\n",
       "New Hampshire        110\n",
       "New Jersey           150\n",
       "New Mexico            15\n",
       "New York             225\n",
       "North Carolina        35\n",
       "Ohio                 125\n",
       "Oklahoma               5\n",
       "Oregon                50\n",
       "Pennsylvania         170\n",
       "Rhode Island           5\n",
       "South Carolina        50\n",
       "South Dakota          50\n",
       "Tennessee             50\n",
       "Texas                180\n",
       "Utah                 175\n",
       "Virginia             135\n",
       "Washington           150\n",
       "West Virginia         20\n",
       "Wisconsin             20\n",
       "other               2335"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_geo_count = long_geotags.groupby('state').count()\n",
    "state_geo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
